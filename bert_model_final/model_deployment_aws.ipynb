{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################################\r\n",
      "##################### IMPORTS Pt.1 ###########################\r\n",
      "###################################################################\r\n",
      "\r\n",
      "# Import to list directories \r\n",
      "import os\r\n",
      "\r\n",
      "print(os.getcwd())\r\n",
      "print(os.listdir(os.getcwd()))\r\n",
      "\r\n",
      "### import for time and time monitoring \r\n",
      "import time\r\n",
      "## Imports for Metrics operations \r\n",
      "import numpy as np \r\n",
      "## Imports for dataframes and .csv managment\r\n",
      "import pandas as pd \r\n",
      "\r\n",
      "# TensorFlow library  Imports\r\n",
      "import tensorflow as tf\r\n",
      "print(\"tf version: \", tf.__version__)\r\n",
      "\r\n",
      "\r\n",
      "os.system('pip install swish-activation')\r\n",
      "\r\n",
      "\r\n",
      "import swish_package\r\n",
      "from swish_package import swish\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# Keras (backended with Tensorflow) Imports\r\n",
      "from tensorflow import keras\r\n",
      "from tensorflow.keras import backend as K\r\n",
      "from tensorflow.keras.models import Sequential, Model\r\n",
      "from tensorflow.keras.layers import Dense, Dropout, Input\r\n",
      "from tensorflow.keras.optimizers import Adam\r\n",
      "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\r\n",
      "from tensorflow.keras.callbacks import CSVLogger\r\n",
      "\r\n",
      "# Sklearn package for machine learining models \r\n",
      "## WILL BE USED TO SPLIT  TRAIN_VAL DATASETS\r\n",
      "from sklearn import metrics\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "\r\n",
      "# Garbage Collectors\r\n",
      "import gc\r\n",
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "## Install Transformers (They are not installed by deafult)\r\n",
      "print(\"running installs...\")\r\n",
      "\r\n",
      "os.system('pip install transformers==2.10')\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# Import Transformers to get Tokenizer for bert and bert models\r\n",
      "\r\n",
      "import transformers\r\n",
      "# from transformers import TFAutoModel, AutoTokenizer\r\n",
      "# from transformers import RobertaTokenizer, TFRobertaModel\r\n",
      "from transformers import DistilBertTokenizer, TFDistilBertModel , RobertaTokenizer\r\n",
      "from tqdm.notebook import tqdm\r\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\r\n",
      "from transformers import XLMRobertaTokenizer ,TFRobertaModel , TFAutoModel\r\n",
      "from transformers import XLMRobertaForSequenceClassification\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "#################################################################\r\n",
      "# HELPER FUNCTIONS\r\n",
      "###################################################################\r\n",
      "print(\"defining helper functions...\")\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "def regular_encode(texts, tokenizer, maxlen=512):\r\n",
      "    enc_di = tokenizer.batch_encode_plus(\r\n",
      "        texts, \r\n",
      "        return_attention_masks=False, \r\n",
      "        return_token_type_ids=False,\r\n",
      "        pad_to_max_length=True,\r\n",
      "        max_length=maxlen\r\n",
      "    )\r\n",
      "    return np.array(enc_di['input_ids'])\r\n",
      "\r\n",
      "\r\n",
      "def build_model(transformer, max_len=512):\r\n",
      "    \"\"\"\r\n",
      "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\r\n",
      "    \"\"\"\r\n",
      "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\r\n",
      "    sequence_output = transformer(input_word_ids)[0]\r\n",
      "    cls_token = sequence_output[:, 0, :]\r\n",
      "    out = Dense(1, activation='sigmoid')(cls_token)\r\n",
      "    \r\n",
      "    model = Model(inputs=input_word_ids, outputs=out)\r\n",
      "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\r\n",
      "    \r\n",
      "    return model\r\n",
      "\r\n",
      "\r\n",
      "def get_train_data():\r\n",
      "    train = pd.read_csv('/content/jigsaw_mjy_train_val_openaug_523200.csv')\r\n",
      "    #test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\r\n",
      "    return train\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat entry_point.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('1', recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "#inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = '2.3',\n",
    "                                  entry_point = 'entry_point.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 1min 30s, sys: 10.6 s, total: 1min 40s\n",
      "Wall time: 8min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.10\n",
      "  Using cached transformers-2.10.0-py3-none-any.whl (660 kB)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Using cached tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers==2.10) (2.22.0)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Processing /home/ec2-user/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2/sacremoses-0.0.43-py3-none-any.whl\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers==2.10) (1.18.1)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers==2.10) (4.42.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers==2.10) (3.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers==2.10) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers==2.10) (1.25.10)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers==2.10) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers==2.10) (2.8)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers==2.10) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers==2.10) (7.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers==2.10) (0.14.1)\n",
      "Installing collected packages: tokenizers, dataclasses, regex, sacremoses, sentencepiece, transformers\n",
      "Successfully installed dataclasses-0.8 regex-2020.11.13 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.7.0 transformers-2.10.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import transformers\n",
    "from transformers.modeling_tf_distilbert import TFDistilBertModel\n",
    "# TensorFlow library  Imports\n",
    "import tensorflow as tf\n",
    "# Keras (backended with Tensorflow) Imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c68b757e15420587c70db30b9d58a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "# Configuration\n",
    "MAX_LEN = 192\n",
    "#MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "#MODEL = 'roberta-base'\n",
    "MODEL = 'distilbert-base-multilingual-cased'\n",
    "EPOCHS = 10\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "D = np.array([\"امشي لساني علي \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = regular_encode(D, tokenizer, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.138935536]]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63807</th>\n",
       "      <td>63807</td>\n",
       "      <td>No, non risponderò, come preannunciato. Prefer...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63808</th>\n",
       "      <td>63808</td>\n",
       "      <td>Ciao, I tecnici della Wikimedia Foundation sta...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63809</th>\n",
       "      <td>63809</td>\n",
       "      <td>innnazitutto ti ringrazio per i ringraziamenti...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63810</th>\n",
       "      <td>63810</td>\n",
       "      <td>Kaç olumlu oy gerekiyor? Şu an 7 oldu.  Hayır...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63811</th>\n",
       "      <td>63811</td>\n",
       "      <td>Te pido disculpas. La verdad es que no me per...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63812 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            content lang\n",
       "0          0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n",
       "1          1   Вполне возможно, но я пока не вижу необходимо...   ru\n",
       "2          2  Quindi tu sei uno di quelli   conservativi  , ...   it\n",
       "3          3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n",
       "4          4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr\n",
       "...      ...                                                ...  ...\n",
       "63807  63807  No, non risponderò, come preannunciato. Prefer...   it\n",
       "63808  63808  Ciao, I tecnici della Wikimedia Foundation sta...   it\n",
       "63809  63809  innnazitutto ti ringrazio per i ringraziamenti...   it\n",
       "63810  63810   Kaç olumlu oy gerekiyor? Şu an 7 oldu.  Hayır...   tr\n",
       "63811  63811   Te pido disculpas. La verdad es que no me per...   es\n",
       "\n",
       "[63812 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tr', 'ru', 'it', 'fr', 'pt', 'es'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.lang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
